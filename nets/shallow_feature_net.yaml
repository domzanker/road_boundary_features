# set False for pre-defined networks
use_custom: True
input_size: [768, 448] # for 4 layer resnet: /112
input_channels: 4
encoder_prec:
        out_channels: 64
        stride: 2
        kernel_size: 7
        dropout: 0.0
        

encoder:
        encoder_depth: 2
        in_channels: [64, 128]
        out_channels: [128, 256]

        kernel_size: 3
        nmbr_res_blocks: 3
        dilation: 2
        batch_norm: True
        activation: "relu"
        dropout: 0.0

decoder:
        decoder_depth: 2
        conv_per_block: 4
        blocks:
                block_1:
                        in_channels: [256, 256, 128, 128]
                        out_channels: [256, 128, 128, 128]
                        kernel_size: [1, 3, 3, 1]
                        stride: 1
                        dilation: 1
                        activation: "relu"
                        dropout: 0.0
                        apply_instance_norm: True
                block_2:
                        in_channels: [128, 128, 64, 64]
                        out_channels: [128, 64, 64, 64]
                        kernel_size: [1, 3, 3, 1]
                        stride: 1
                        dilation: 1
                        activation: "relu"
                        upsampling: 4
                        dropout: 0.0
                        apply_instance_norm: True
head: 
        scale_factor: 2
        branches:

                distance_branch:
                        in_channels: [64, 32, 16]
                        out_channels: [32, 16, 1]
                        kernel_sizes: [3, 3, 1]
                        strides: 1
                        dilation: 1
                        batch_norm: True
                        end_activation: "relu"
                        dropout: 0.0
                end_point_branch:
                        in_channels: [64, 32, 16]
                        out_channels: [32, 16, 1]
                        kernel_sizes: [3, 3, 1]
                        strides: 1
                        dilation: 1
                        batch_norm: True
                        end_activation: "relu"
                        dropout: 0.0
                direction_branch:
                        in_channels: [64, 32, 16]
                        out_channels: [32, 16, 2]
                        kernel_sizes: [3, 3, 1]
                        strides: 1
                        dilation: 1
                        batch_norm: True
                        end_activation: "tanh"
                        dropout: 0.0
